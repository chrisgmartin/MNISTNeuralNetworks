{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Image Recognition Neural Network\n",
    "#### Chris G Martin\n",
    "#### CUNY DATA622 - Final Project\n",
    "#### December 10, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project was created to design a neural network to identify handwritten numbers from the [MNIST dataset](http://yann.lecun.com/exdb/mnist/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of the code and logic used in the first two neural networks come from [Nueral Networks and Deep Learning by Michael Nielsen](http://neuralnetworksanddeeplearning.com/), with modifications and explainations on assumptions and hyper-parameters provided by me. The third network was an experimental one attempting to use Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "Let's start by loading the MNIST data, using the pull steps from the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Used the data from Nielsen's GitHub simply for (1) reproduction and (2) ease of use.\n",
    "    \n",
    "    Training set has 50,000 entries,\n",
    "    Validation set has 10,000 entries,\n",
    "    And test set has 10,000 entries\n",
    "    \"\"\"\n",
    "    f = gzip.open('C:/Users/itsal/Documents/CUNY/DATA622/Week 15-16/mnist.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = cPickle.load(f)\n",
    "    f.close()\n",
    "    return(training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    \"\"\"\n",
    "    Convert inputs to vector\n",
    "    Starts by creating empty vector of zeros, one for each digit 0-9       \n",
    "    For each input j in vector e, label the digits as their binary digit type\n",
    "    For exampe, a 2 digit returns vector [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "    \"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "def load_data_wrapper():\n",
    "    \"\"\"\n",
    "    Training data = tr_d\n",
    "    Validation data = va_d\n",
    "    Test data = te_d\n",
    "    \n",
    "    This function reshapes the datasets into a training dataset with a corresponding vector\n",
    "    of images and a vector with respective binary result indicator. See Neilsen Ch 1\n",
    "    for reasoning behind format variations of the three sets.\n",
    "    \n",
    "    There are 784 pixel inputs (28 x 28).\n",
    "    \"\"\"\n",
    "    tr_d, va_d, te_d = load_data()    \n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    #training digit results reshape\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = zip(training_inputs, training_results)    \n",
    "    #validation data reshape\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = zip(validation_inputs, va_d[1])\n",
    "    #test data reshape\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = zip(test_inputs, te_d[1])\n",
    "    return(training_data, validation_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = load_data_wrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network 1: Stochastic Graident Decent, Cross-Entropy, and Regularization\n",
    "This neural network utilizes concepts from Chapters 1-3 (Neilsen). It implements stochastic gradient decent for a feedforward neural network, backpropegation, cost estimated by cross-entropy, and regularization. Some experimentation was performed (described in the sections below) to establish hyper-parameters and network weight tuning, so results may vary (for better or, more likely, for worse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Method, Backpropegation, & Sigmoid\n",
    "As mentioned, this neural network uses a feedforward method, backpropegation, and sigmoid. Sigmoid is an activation function with inputs (x), weights (w), and bias (b) and can be defined as:\n",
    "\n",
    "$\\sigma(z) = \\frac{1}{1 + e^{-z}} = \\frac{1}{1 + exp(- \\sum_{j} w_{j} x_{j} - b)}$\n",
    "\n",
    "with\n",
    "\n",
    "$z = (w * a) + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One downside of sigmoid vs other activation functions (i.e. ReLU, Softmax, or tanh) is that activations > |1| lead to a gradient near, if not exactly, zero. This is known as gradient saturation -- when the outputs of the layers in the feedforward pass give an activation too high to backprop a meaningful graident, and restricts the network from learning. The other activation functions also have their own similar or different limitations.\n",
    "\n",
    "In fact, ReLU (rectified linear units) is shown in Nielsen to be more accurate, and tanh can also improve the network. I will add these functions in the following network, but focus now on sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code to use the feedforward method and sigmoid is defined in the network function below as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def feedforward(self, a):<br>\n",
    "    for b, w in zip(self.biases, self.weights):<br>\n",
    "        a = sigmoid(np.dot(w, a)+b):<br>\n",
    "    return a:<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for backpropegation can be found in the full network code at the end of this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Decent (SGD)\n",
    "\n",
    "In determining our appropriate weights and biases, we use Stochastic Gradient Decent (SGD) to find a set of weight and biases reducing the cost function (or loss function, or obejective function) of our network as low as possible. SGD uses batches (mini batches) to sample the training set for parameters, vs Gradient Descent's (GD) use of the entire sample. The benefit of using SGD in the MNIST dataset is that there are a fairly decent sample size in the training set (50,000 images), so batching the samples into smaller pieces can speed up the network's learning speed. If there was only a small sample of training data, perhaps it GD would be a more accurate and useful method but using it on the full MNIST datset would add a significant amount of time to run our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for SGD can be found in the full network code at the end of this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Batching\n",
    "\n",
    "As mentioned, SGD uses batches to reduce the time for weight and bias estimations in optimizing our calculation for cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Entropy Cost\n",
    "Backpropagation is used to compute partial derivatives $\\frac{\\partial C}{\\partial b}$ and $\\frac{\\partial C}{\\partial w}$ of the cost function (C) in respect to any bias (b) or weight (w). One method of calculating the costs is the quadratic cost, but we will use the cross-entropy cost method for reasons described here. In the case of quadratic cost estimation, the estimation of the partial derivatives are:\n",
    "\n",
    "$\\frac{\\partial C}{\\partial w} = (a - y)\\sigma'(z)x = a \\sigma'(z)$\n",
    "\n",
    "$\\frac{\\partial C}{\\partial b} = (a - y)\\sigma'(z) = a \\sigma'(z)$\n",
    "\n",
    "as\n",
    "\n",
    "$a = \\sigma(z)$ where $z = wx + b$\n",
    "\n",
    "The rate at which a neuron learns is controlled by $\\sigma(z) - y$ (= the size of the error of the output); the larger the error, the faster the neuron will learn. Cross-entropy solves this learning slowdown since the $\\sigma'(z)$ term is canclled out and the estimation of the partial derivatives are:\n",
    "\n",
    "$\\frac{\\partial C}{\\partial w} = \\frac{1}{n}\\sum_{x}x_{j}(\\sigma(z) - y)$\n",
    "\n",
    "$\\frac{\\partial C}{\\partial b} = \\frac{1}{n}\\sum_{x}(\\sigma(z) - y)$\n",
    "\n",
    "While the quardratic cost function has the form $C = \\frac{1}{2 n} \\sum_{x} ||y(x) - a^L(x)||^2$, an individual quadradic cost for a single training example (x) is $C = \\frac{1}{2}\\sum_{j}(y_{j} - a_{j}^L)^2$. n = total number of training examples, x = individual training example, y the corresponding desired output, L is the number of layers in the network, and $a^L$ is the vector of activations output from the network when x is input.\n",
    "\n",
    "The cost estimate using cross-entry is:\n",
    "\n",
    "$C = -\\frac{1}{n}\\sum_{x}[y ln a + (1 - y) ln(1 - a)]$\n",
    "\n",
    "Cross-Entropy should speed the learning of our network, and theoretically give us better results than the quadratic method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CrossEntropyCost(object):\n",
    "    \"\"\"\n",
    "    Class for cost-entrophy cost: class is used to return two different measures\n",
    "    (1) CrossEntropyCost.fn computes the cost, how well output activation (a) matches expected output (y)\n",
    "    (2) CrossEntropyCost.delta computes the delta of the output error\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        return np.sum(np.nan_to_num(-y * np.log(a) - (1-y) * np.log(1-a)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        return (a-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "Regularization is a method to reduce overfitting. The regularization technique used in this model is L2 regularization, in which an extra term (regularization term) is added to the cost function. This technique is also known as weight decay for reasons shown here. For our cross-entropy cost we can see the change as:\n",
    "\n",
    "$C = -\\frac{1}{n} \\sum_{xj} [y_{j} ln a_{j}^L + (1 - y_{j})(ln(1 - a_{j}^L)] + \\frac{\\lambda}{2n} \\sum_{w} w^2$\n",
    "\n",
    "With our regularization term:\n",
    "\n",
    "$\\frac{\\lambda}{2n} \\sum_{w} w^2$\n",
    "\n",
    "More simply, regularization changes cost in general as:\n",
    "\n",
    "$C = C_{0} + \\frac{\\lambda}{2n} \\sum_{w} w^2$\n",
    "\n",
    "As it can be seen, $\\lambda$ is an important term since the value of can determine the impact regularization has to cost. With a large $\\lambda$, small weights are prefered since the impact on cost is reduced. With a small $\\lambda$, larger weights are preferred since it will impact the cost. The partial dervivatives of $\\frac{\\partial C}{\\partial b}$ and $\\frac{\\partial C}{\\partial w}$ now become (for use in backpropegation):\n",
    "\n",
    "$\\frac{\\partial C}{\\partial w} = \\frac{\\partial C_{0}}{\\partial w} + \\frac{\\lambda}{n}w$\n",
    "\n",
    "$\\frac{\\partial C}{\\partial b} = \\frac{\\partial C_{0}}{\\partial b}$\n",
    "\n",
    "There is no change to bias, but with gradient descent (and, more importantly, SGD) our weight begins to rescale (decay) at a factor of $1 - \\frac{\\eta \\lambda}{n}.\n",
    "\n",
    "Other regularization types include L1 regularization (which changes the addition to cost of $\\frac{\\lambda}{n}\\sum{w}$ the absolute value of weights $\\frac{\\lambda}{n}\\sum{w}$, effectively changing the impact of extremely large/small weights.\n",
    "\n",
    "Dropout is a method of regularization by which neurons in the network are randomly (and temporarily) dropped from the network during forward-propegation and backpropegation, and the pattern is continously repeated. Weights and biases are thusly changed under different conditions and (presumably) averaged.\n",
    "\n",
    "Artifically expanding the training set is also an interesting method, as the training set can be increased by slightly modifying the inputs. For our MNIST example, we can increase the training set by slightly 'tilting' our images (i.e. 15 degrees), or osciliating generated digits (to mimic hand spasms), or skewing images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for regularization can be found in the full network code at the end of this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Initialization\n",
    "The topic of weight initialization was quickly covered by Nielsen, so outside sources such as a blog post by [Andre Perunicic on Neural Network Weight Initialization](https://intoli.com/blog/neural-network-initialization/), a [StackExchange question on Initial Weights](https://intoli.com/blog/neural-network-initialization/), a blog post by [Isaac Changhau called Weight Initialization in Artificial Neural Networks](https://isaacchanghau.github.io/2017/05/24/Weight-Initialization-in-Artificial-Neural-Networks/), a write-up by [Siddharth Krishna Kumar at Cornell University](https://arxiv.org/abs/1704.08863), and a post on [Xavier initialization by Prateek Joshi](https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/)  served as side-research into the subject.\n",
    "\n",
    "One apparently popular method of initlization is Xavier initilization. A Guassian distribution has a mean of zero and a specific, finance variance. An Xavier initiliazation itiliazes the weights across the layers and keeps the variance through each layer the same, through a Gaussian distribution. The forumla for the variance using backpropegation would be:\n",
    "\n",
    "$Var(W_{i}) = \\frac{2}{n_{in} + n_{out}}$\n",
    "\n",
    "with a more simplified (and possibly refined?) forumla being:\n",
    "\n",
    "$Var(W) = \\frac{2}{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for weight initialization can be found in the full network code at the end of this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameter Tuning\n",
    "Hyper parameters are extremely important to the model, and tuning can be performed in a number of ways. I can even imagine a quite simple Monte Carlo function to run several experiements on tuning the parameters until an 'optimal' parameter is found (or, the best of the rest parameters which seems more likely). These parameters could be the difference \n",
    "\n",
    "The primary parameters I adjusted were $\\eta$ and $\\lambda$, by running the network several times and reducing the number of samples in our evaluation data and training data sets to 100 and 1000, respectively. Reducing the number of samples allowed me to quickly tune the parameters, since the network was significantly sped up due to reduced input sizes. Tuning was performed by first setting the parameters to extremes (such as $\\eta$=1000 and $\\lambda$=1000, then reducing or increasing the parameter until it seems like a good fit, then switching to the other parameter and increasing or reducing the parameter until a good fit seemed in range, then rinse and repeat.\n",
    "\n",
    "A major obstacle I found was that I could find parameters that fit the training data very well (~99% accuracy), but the evaulation set was far lower (~80% accuracy) which indicated overfitting. I also found that spme well performing parameters on the shrunked set did not perform well with the full set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final hyper parameters are set in the code to run the newtork as shown at the end of this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of the Network and Total Cost\n",
    "The next question we need to ask is, how do we track the performance of our network? Accuracy can be tracked by counting the number of correctly identified images, while total cost is monitored as a means for efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for accuracy and total cost can be found in the full network code at the end of this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "Using the data loaded early, the CrossEntropyCost class defined earlier, the sigmoid and sigmoid prime functions, and the discussions above, here is the neural network, with minor modifications to the [Nielsen code (network2.py)](https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network2.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "class Network1(object):\n",
    "    \n",
    "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
    "        \"\"\"\n",
    "        Sizes contains the number of neurons in the respective layers of the network.\n",
    "        For example [2, 3, 1] is a three-layer network with 2 neurons in the first layer,\n",
    "        3 in the second layer, and 1 neuron in the third layer.\n",
    "        \n",
    "        xavier_initilizer initilizes the biases and weights of the network randomly.\n",
    "        default_weight_initializer uses Gaussian distribution.\n",
    "        large_weight_initilizer standardize or does not reduce the weights.\n",
    "        These are all included for comparison\n",
    "        \"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.xavier_initilizer()\n",
    "        self.cost = cost\n",
    "        \n",
    "    def xavier_initilizer(self):\n",
    "        \"\"\"\n",
    "        Initialize weights using a Gaussian distribution with mean 0.\n",
    "        The first layer is assumed to be an input layer, so no bias is set.\n",
    "        \"\"\"\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [(np.random.randn(y, x)*2)/(np.sqrt(x))\n",
    "                       for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def default_weight_initializer(self):\n",
    "        \"\"\"\n",
    "        Initialize weights using a Gaussian distribution with mean 0 std of 1.\n",
    "        The first layer is assumed to be an input layer, so no bias is set.\n",
    "        \"\"\"\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)/np.sqrt(x)\n",
    "                       for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def large_weight_initilizer(self):\n",
    "        \"\"\"\n",
    "        Initialize weights using a Gaussian distribution with mean 0.\n",
    "        The first layer is assumed to be an input layer, so no bias is set.\n",
    "        \"\"\"\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                       for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"\n",
    "        Feed forward function, using sigmoid.\n",
    "        \"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, lmbda = 0.0,\n",
    "            evaluation_data=None, monitor_evaluation_cost=False, monitor_evaluation_accuracy=False,\n",
    "            monitor_training_cost=False, monitor_training_accuracy=False):\n",
    "        \"\"\"\n",
    "        Stochastic Gradient Descent using batches as defined by mini_batch_size.\n",
    "        Evaluation data is optional, and can be included when the network is run.\n",
    "        lmbda is the regularization parameter.\n",
    "        Cost and accuracy can be tracked by marking 'True' for evalution and training data\n",
    "        with costs per epoch, with the first tuple being the list at the end of the epoch.\n",
    "        \"\"\"\n",
    "        if evaluation_data: n_data = len(evaluation_data)\n",
    "        n = len(training_data)\n",
    "        #create empty vectors to be filled in if monitors are true\n",
    "        evaluation_cost, evaluation_accuracy = [], []\n",
    "        training_cost, training_accuracy = [], []\n",
    "        for j in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in xrange(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta, lmbda, len(training_data))\n",
    "            print \"Epoch %s training complete\" % j\n",
    "            if monitor_training_cost:\n",
    "                cost = self.total_cost(training_data, lmbda)\n",
    "                training_cost.append(cost)\n",
    "                print \"Cost on training data: {}\".format(cost)\n",
    "            if monitor_training_accuracy:\n",
    "                accuracy = self.accuracy(training_data, convert=True)\n",
    "                training_accuracy.append(accuracy)\n",
    "                print \"Accuracy on training data: {} / {}\".format(accuracy, n)\n",
    "            if monitor_evaluation_cost:\n",
    "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
    "                evaluation_cost.append(cost)\n",
    "                print \"Cost on evaluation data: {}\".format(cost)\n",
    "            if monitor_evaluation_accuracy:\n",
    "                accuracy = self.accuracy(evaluation_data)\n",
    "                evaluation_accuracy.append(accuracy)\n",
    "                print \"Accuracy on evaluation data: {} / {}\".format(self.accuracy(evaluation_data), n_data)\n",
    "            print\n",
    "        return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
    "        \"\"\"\n",
    "        Updates the network's weights and biases by applying GD using backpropegation to a single mini batch.\n",
    "        The mini_batch is a list of tuples (x,y).\n",
    "        eta is the learning rate.\n",
    "        n is the total size of the training data set.\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x,y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.baises = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
    "        \n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"\n",
    "        Returns a tuple representing the gradient for the cost function C_x.\n",
    "        nabla_b and nabla_w is a list of arrays for each layer.\n",
    "        \"\"\"\n",
    "        #b=bias, w=weight\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        #feedforward\n",
    "        activation = x\n",
    "        #store all activations by layer\n",
    "        activations = [x]\n",
    "        #store all z vectors by layer\n",
    "        zs = []\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            #z = w * a + b\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        #backward pass\n",
    "        #-1 indicates the last value\n",
    "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            #-l as in L, not 1\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def accuracy(self, data, convert=False):\n",
    "        \"\"\"\n",
    "        Returns the correct number of inputs identified by the network.\n",
    "        Convert should be set to False if the data set is validation or test data, and True if training data.\n",
    "        As seen in the data load, y is different in the training data set, so conversion is made here.\n",
    "        \"\"\"\n",
    "        if convert:\n",
    "            results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
    "                      for (x, y) in data]\n",
    "        else:\n",
    "            results = [(np.argmax(self.feedforward(x)), y)\n",
    "                      for (x, y) in data]\n",
    "        return sum(int(x == y) for (x, y) in results)\n",
    "    \n",
    "    def total_cost(self, data, lmbda, convert=False):\n",
    "        \"\"\"\n",
    "        Returns the total cost for the data set in data. See 'accuracy' for convert.\n",
    "        \"\"\"\n",
    "        cost = 0.0\n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x)\n",
    "            if convert: y = vectorized_result(y)\n",
    "            cost += self.cost.fn(a, y)/len(data)\n",
    "        cost += 0.5 * (lmbda/len(data)) * sum(np.linalg.norm(w)**2 for w in self.weights)\n",
    "        return cost\n",
    "    \n",
    "    def save(self, filename):\n",
    "        \"\"\"\n",
    "        Saves the neural network to the file 'filename'.\n",
    "        \"\"\"\n",
    "        data = {\"sizes\": self.sizes,\n",
    "               \"weights\": [w.tolist() for w in self.weights],\n",
    "               \"biases\": [b.tolist() for b in self.biases],\n",
    "               \"cost\": str(self.cost.__name__)}\n",
    "        f = open(filename, \"w\")\n",
    "        json.dump(data, f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Network\n",
    "With our network built, we can run it and see how it performs. Remember, our data was already loaded at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Network1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-436489ee16bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNetwork1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#, cost=CrossEntropyCost)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m evaluation_cost, evaluation_accuracy, training_cost, training_accuracy = net.SGD(\n\u001b[0;32m      3\u001b[0m     \u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.156\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlmbda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[0mmonitor_evaluation_accuracy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonitor_evaluation_cost\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             monitor_training_accuracy=True, monitor_training_cost=True)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Network1' is not defined"
     ]
    }
   ],
   "source": [
    "net = Network1([784, 10])#, cost=CrossEntropyCost)\n",
    "evaluation_cost, evaluation_accuracy, training_cost, training_accuracy = net.SGD(\n",
    "    training_data, 40, 10, 0.156, lmbda=2.2, evaluation_data=validation_data,\n",
    "            monitor_evaluation_accuracy=True, monitor_evaluation_cost=True,\n",
    "            monitor_training_accuracy=True, monitor_training_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the results, it looks like I was able to get an accuracy on the training data of ~92% correct identification, and evaluation accuracy of ~92%. This is pretty good, but not even close to the results Nielsen obtained (~98%). The weakness could be in our hyper parameters, xavier initialization, or epoch sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network 2: Theanos Convolutional Neural Network\n",
    "The Convolutional Neural Network expands the neural network by piecing together pixels (for image data like what we're using here) into a feature map representing block images. This particular neural network uses pooling methods to create hidden layers of pooled pixel inputs. Most of the code here is from Nielsen's book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "As mentioned in the previous network description, ReLU and tanh activation may outperform the sigmoid activation function we used. Here we will define both functions, and add linear activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "\n",
    "def linear(z): return z\n",
    "def ReLU(z): return T.maximum(0.0, z)\n",
    "\n",
    "#pre-defined sigmoid activation\n",
    "from theano.tensor.nnet import sigmoid\n",
    "#pre-defined tanh activation\n",
    "from theano.tensor import tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU, Parallel, and Cloud Processing\n",
    "One advantage of using Theano is that it enables GPU to speed up processing along with CPU. Other ways to improve processing speeds are to use parallel processing techniques or cloud processing. For the sake of simplicity and time (the irony, I know) and due to the limitations of my laptop (which is on its last legs), I will simply use CPU processing but add the option for GPU here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with a CPU. If this is not desired then set GPU to True.\n"
     ]
    }
   ],
   "source": [
    "GPU = False\n",
    "if GPU:\n",
    "    print \"Trying to run under GPU. If this is not desired then set GPU to False.\"\n",
    "    try: theano.config.device = 'gpu'\n",
    "    except: pass\n",
    "    theano.config.floatX = 'float32'\n",
    "else:\n",
    "    print \"Running with CPU. If this is not desired then set GPU to True.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Variables\n",
    "The data set has already been loaded in the beginning, but Theano uses shared variables allowing the data to be copied to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano.tensor as T\n",
    "\n",
    "def load_data_shared(filename=\"C:/Users/itsal/Documents/CUNY/DATA622/Week 15-16/mnist.pkl.gz\"):\n",
    "    f = gzip.open(filename, 'rb')\n",
    "    training_data, validation_data, test_data = cPickle.load(f)\n",
    "    f.close()\n",
    "    def shared(data):\n",
    "        \"\"\"\n",
    "        Shared variables\n",
    "        \"\"\"\n",
    "        shared_x=theano.shared(np.asarray(data[0], dtype=theano.config.floatX), borrow=True)\n",
    "        shared_y=theano.shared(np.asarray(data[1], dtype=theano.config.floatX), borrow=True)\n",
    "        return shared_x, T.cast(shared_y, \"int32\")\n",
    "    return [shared(training_data), shared(validation_data), shared(test_data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from theano.tensor.nnet import conv2d\n",
    "from theano.tensor.nnet import softmax\n",
    "from theano.tensor import shared_randomstreams\n",
    "from theano.tensor.signal import pool\n",
    "\n",
    "\n",
    "class Network2(object):\n",
    "    \n",
    "    def __init__(self, layers, mini_batch_size):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.params = [param for layer in self.layers for param in layer.params]\n",
    "        self.x = T.matrix(\"x\")\n",
    "        self.y = T.ivector(\"y\")\n",
    "        init_layer = self.layers[0]\n",
    "        init_layer.set_inpt(self.x, self.x, self.mini_batch_size)\n",
    "        for j in xrange(1, len(self.layers)):\n",
    "            prev_layer, layer = self.layers[j-1], self.layers[j]\n",
    "            layer.set_inpt(prev_layer.output, prev_layer.output_dropout, self.mini_batch_size)\n",
    "        self.output = self.layers[-1].output\n",
    "        self.output_dropout = self.layers[-1].output_dropout\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, validation_data, test_data, lmbda=0.0):\n",
    "        \"\"\"\n",
    "        Stochastic Gradient Descent with mini-batches\n",
    "        \"\"\"\n",
    "        training_x, training_y = training_data\n",
    "        validation_x, validation_y = validation_data\n",
    "        test_x, test_y = test_data\n",
    "        #number of mini-batches for each type of data\n",
    "        num_training_batches = size(training_data)/mini_batch_size\n",
    "        num_validation_batches = size(validation_data)/mini_batch_size\n",
    "        num_test_batches = size(test_data)/mini_batch_size\n",
    "        #regularized cost, symbolic gradients, and updates\n",
    "        l2_norm_squared = sum([(layer.w**2).sum() for layer in self.layers])\n",
    "        cost = self.layers[-1].cost(self) + 0.5 * lmbda * l2_norm_squared / num_training_batches\n",
    "        grads = T.grad(cost, self.params)\n",
    "        updates = [(param, param-eta*grad) for param, grad in zip(self.params, grads)]\n",
    "        #compute accruacy in validation and test mini-batches\n",
    "        i = T.lscalar()\n",
    "        \n",
    "        train_mb = theano.function(\n",
    "            [i], cost, updates=updates,\n",
    "            givens={\n",
    "                self.x:\n",
    "                training_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
    "                self.y:\n",
    "                training_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
    "            })\n",
    "\n",
    "        #train_mb = theano.function([i], cost, updates=updates, givens={\n",
    "        #        self.x: training_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
    "        #        self.y: training_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]})\n",
    "        validate_mb_accuracy = theano.function([i], self.layers[-1].accuracy(self.y), givens={\n",
    "                self.x: validation_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
    "                self.y: validation_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]})\n",
    "        test_mb_accuracy = theano.function([i], self.layers[-1].accuracy(self.y), givens={\n",
    "                self.x: test_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
    "                self.y: test_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]})\n",
    "        self.test_mb_predictions = theano.function([i], self.layers[-1].y_out, givens={\n",
    "                self.x: test_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size]})\n",
    "        #training\n",
    "        best_validation_accuracy = 0.0\n",
    "        for epoch in xrange(epochs):\n",
    "            for minibatch_index in xrange(num_training_batches):\n",
    "                iteration = num_training_batches*epoch + minibatch_index\n",
    "                if iteration % 1000 == 0:\n",
    "                    print(\"Training mini-batch number [0]\".format(iteration))\n",
    "                cost_ij = train_mb(minibatch_index)\n",
    "                if (iteration+1) % num_training_batches == 0:\n",
    "                    validation_accuracy = np.mean([validate_mb_accuracy(j) for j in xrange(num_validation_batches)])\n",
    "                    print(\"Epoch [0]: validation accuracy {1:.2%}\".format(epoch, validation_accuracy))\n",
    "                    if validation_accuracy >= best_validation_accuracy:\n",
    "                        print(\"This is the best validation accuracy to date.\")\n",
    "                        best_validation_accuracy = validation_accuracy\n",
    "                        best_iteration = iteration\n",
    "                        if test_data:\n",
    "                            test_accuracy = np.mean([test_mb_accuracy(j) for j in xrange(num_test_batches)])\n",
    "                            print(\"The corresponding test accuracy is {0:.2%}\".format(test_accuracy))\n",
    "            print(\"Finished training network.\")\n",
    "            print(\"Best validation accuracy of {0:.2%} obtained at iteration {1}\".format(best_validation_accuracy, best_iteration))\n",
    "            print(\"Corresponding test accuracy of {0:.2%}\".format(test_accuracy))\n",
    "\n",
    "class ConvPoolLayer(object):\n",
    "    \"\"\"\n",
    "    Creates a convolutional layer and max-pooling layer.\n",
    "    The two layers are combined into a single class for consistency with Nielsen.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filter_shape, input_shape, poolsize=(2,2), activation_fn=sigmoid):\n",
    "        \"\"\"\n",
    "        Default using a pool of 2x2 and sigmoid activation\n",
    "        \"\"\"\n",
    "        self.filter_shape = filter_shape\n",
    "        self.input_shape = input_shape\n",
    "        self.poolsize = poolsize\n",
    "        self.activation_fn = activation_fn\n",
    "        #initialization\n",
    "        n_out = (filter_shape[0]*np.prod(filter_shape[2:])/np.prod(poolsize))\n",
    "        self.w = theano.shared(np.asarray(np.random.normal(loc=0, scale=np.sqrt(1.0/n_out), size=filter_shape),\n",
    "                                         dtype=theano.config.floatX),\n",
    "                              borrow=True)\n",
    "        self.b = theano.shared(np.asarray(np.random.normal(loc=0, scale=1.0, size=(filter_shape[0],)),\n",
    "                                         dtype=theano.config.floatX),\n",
    "                              borrow=True)\n",
    "        self.params = [self.w, self.b]\n",
    "        \n",
    "    def set_inpt(self, inpt, inpt_dropout, mini_batch_size):\n",
    "        self.inpt = inpt.reshape(self.input_shape)\n",
    "        conv_out = conv2d(input=self.inpt, filters=self.w, filter_shape=self.filter_shape,\n",
    "                              input_shape=self.input_shape)\n",
    "        pooled_out = pool.pool_2d(input=conv_out, ws=self.poolsize, ignore_border=True)\n",
    "        self.output = self.activation_fn(pooled_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n",
    "        self.output_dropout = self.output\n",
    "\n",
    "class FullyConnectedLayer(object):\n",
    "    \"\"\"\n",
    "    Fully Connected Layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_in, n_out, activation_fn=sigmoid, p_dropout=0.0):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.activation_fn = activation_fn\n",
    "        self.p_dropout = p_dropout\n",
    "        #initalizes weights and biases\n",
    "        self.w = theano.shared(\n",
    "            np.asarray(\n",
    "                np.random.normal(\n",
    "                    loc=0.0, scale=np.sqrt(1.0/n_out), size=(n_in, n_out)),\n",
    "                dtype=theano.config.floatX),\n",
    "            name='w', borrow=True)\n",
    "        self.b = theano.shared(\n",
    "            np.asarray(np.random.normal(loc=0.0, scale=1.0, size=(n_out,)),\n",
    "                       dtype=theano.config.floatX),\n",
    "            name='b', borrow=True)\n",
    "\n",
    "        #self.w = theano.shared(np.asarray(np.random.normal(loc=0.0, scale=np.sqrt(1/n_out), size=(n_in, n_out)),\n",
    "        #                                  dtype=theano.config.floatX),\n",
    "        #                       name='w', borrow=True)\n",
    "        #self.b = theano.shared(np.asarray(np.random.normal(loc=0.0, scale=1.0, size=(n_out,)),\n",
    "        #                                 dtype=theano.config.floatX),\n",
    "        #                       name='b', borrow=True)\n",
    "        self.params = [self.w, self.b]\n",
    "    \n",
    "    def set_inpt(self, inpt, inpt_dropout, mini_batch_size):\n",
    "        \"\"\"\n",
    "        input\n",
    "        \"\"\"\n",
    "        self.inpt = inpt.reshape((mini_batch_size, self.n_in))\n",
    "        self.output = self.activation_fn((1-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)\n",
    "        self.y_out = T.argmax(self.output, axis=1)\n",
    "        self.inpt_dropout = dropout_layer(inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)\n",
    "        self.output_dropout = self.activation_fn(T.dot(self.inpt_dropout, self.w) + self.b)\n",
    "    \n",
    "    def accuracy(self, y):\n",
    "        \"\"\"\n",
    "        Accuracy of mini-batch\n",
    "        \"\"\"\n",
    "        return T.mean(T.eq(y, self.y_out))\n",
    "\n",
    "class SoftmaxLayer(object):\n",
    "    \"\"\"\n",
    "    Softmax Layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_in, n_out, p_dropout=0.0):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.p_dropout = p_dropout\n",
    "        #initializes wights and biases\n",
    "        self.w = theano.shared(np.zeros((n_in, n_out), dtype=theano.config.floatX), name='w', borrow=True)\n",
    "        self.b = theano.shared(np.zeros((n_out,), dtype=theano.config.floatX), name='b', borrow=True)\n",
    "        self.params = [self.w, self.b]\n",
    "    \n",
    "    def set_inpt(self, inpt, inpt_dropout, mini_batch_size):\n",
    "        \"\"\"\n",
    "        input\n",
    "        \"\"\"\n",
    "        self.inpt = inpt.reshape((mini_batch_size, self.n_in))\n",
    "        self.output = softmax((1-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)\n",
    "        self.y_out = T.argmax(self.output, axis=1)\n",
    "        self.inpt_dropout = dropout_layer(inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)\n",
    "        self.output_dropout = softmax(T.dot(self.inpt_dropout, self.w) + self.b)\n",
    "    \n",
    "    def cost(self, net):\n",
    "        \"\"\"\n",
    "        Returns the cost (log-likelihood).\n",
    "        \"\"\"\n",
    "        return -T.mean(T.log(self.output_dropout)[T.arange(net.y.shape[0]), net.y])\n",
    "    \n",
    "    def accuracy(self, y):\n",
    "        \"\"\"\n",
    "        Returns the accuracy of the mini-batch.\n",
    "        \"\"\"\n",
    "        return T.mean(T.eq(y, self.y_out))\n",
    "\n",
    "def size(data):\n",
    "    \"\"\"\n",
    "    Size of the data set.\n",
    "    \"\"\"\n",
    "    return data[0].get_value(borrow=True).shape[0]\n",
    "\n",
    "def dropout_layer(layer, p_dropout):\n",
    "    \"\"\"\n",
    "    Dropout Layer\n",
    "    \"\"\"\n",
    "    srng = shared_randomstreams.RandomStreams(np.random.RandomState(0).randint(999999))\n",
    "    mask = srng.binomial(n=1, p=1-p_dropout, size=layer.shape)\n",
    "    return layer*T.cast(mask, theano.config.floatX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Network\n",
    "The code above allows us to specify the layers we would like to add, the type of activation in the layers, and the mini-batch size, as well as several other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = load_data_shared()\n",
    "mini_batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:114: UserWarning: The `image_shape` keyword argument to `tensor.nnet.conv2d` is deprecated, it has been renamed to `input_shape`.\n",
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\theano\\tensor\\nnet\\conv.py:98: UserWarning: theano.tensor.nnet.conv.conv2d is deprecated. Use theano.tensor.nnet.conv2d instead.\n",
      "  warnings.warn(\"theano.tensor.nnet.conv.conv2d is deprecated.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mini-batch number [0]\n"
     ]
    }
   ],
   "source": [
    "net = Network2([\n",
    "        ConvPoolLayer(input_shape=(mini_batch_size, 1, 28, 28),\n",
    "                     filter_shape=(20, 1, 5, 5),\n",
    "                     poolsize=(2,2),\n",
    "                     activation_fn=ReLU),\n",
    "        ConvPoolLayer(input_shape=(mini_batch_size, 20, 12, 12),\n",
    "                     filter_shape=(40, 20, 5, 5),\n",
    "                     poolsize=(2,2),\n",
    "                     activation_fn=ReLU),\n",
    "        FullyConnectedLayer(n_in=40*4*4, n_out=1000, activation_fn=ReLU, p_dropout=0.5),\n",
    "        SoftmaxLayer(n_in=1000, n_out=10, p_dropout=0.5)],\n",
    "              mini_batch_size)\n",
    "\n",
    "net.SGD(training_data, 60, mini_batch_size, 0.03, validation_data, test_data, lmbda=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network ran extremely slowly, and I blame the quality of my computer and personal impatience. Debugging fixed several errors, and fixing depreciating/depreciated theanos modules fixed several more. As such, I did not change the hyper parameters as used in Nielsen. My best performance marked 97.7% accuracy. A far better result than my previous attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network 3: Keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Network\n",
    "The functioned defined below can be used to load a saved neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def load(filename):\n",
    "    \"\"\"\n",
    "    Load a neural network from the file 'filename'.\n",
    "    \"\"\"\n",
    "    f = open(fileame, \"r\")\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
    "    net = Network(data[\"sizes\"], cost=cost)\n",
    "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
    "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
    "    return net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
